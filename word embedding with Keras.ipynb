{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Building the Emojifier-V2\n",
    "\n",
    "Lets now build the Emojifier-V2 model. You will do so using the embedding layer you have built, and feed its output to an LSTM network. \n",
    "\n",
    "<img src=\"images/emojifier-v2.png\" style=\"width:700px;height:400px;\"> <br>\n",
    "<caption><center> **Figure 3**: Emojifier-v2. A 2-layer LSTM sequence classifier. </center></caption>\n",
    "\n",
    "\n",
    "**Exercise:** Implement `Emojify_V2()`, which builds a Keras graph of the architecture shown in Figure 3. The model takes as input an array of sentences of shape (`m`, `max_len`, ) defined by `input_shape`. It should output a softmax probability vector of shape (`m`, `C = 5`). You may need `Input(shape = ..., dtype = '...')`, [LSTM()](https://keras.io/layers/recurrent/#lstm), [Dropout()](https://keras.io/layers/core/#dropout), [Dense()](https://keras.io/layers/core/#dense), and [Activation()](https://keras.io/activations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from emo_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embedding\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81641\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index['bottle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottle\n"
     ]
    }
   ],
   "source": [
    "print(index_to_word[81641])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20635   0.2606   -0.094723 -0.73396   0.72598   0.5099   -0.39352\n",
      " -0.45703   0.49335   1.3791    0.10285   0.14997   0.41506  -0.19039\n",
      "  1.0527    0.16514  -0.16717   0.8092   -0.97394  -1.753     0.34632\n",
      " -0.053064  0.33046  -0.021036 -0.78655  -1.0088   -0.30341   1.6766\n",
      "  0.90808  -0.39309   1.2131    0.21588  -0.87778   1.3756    0.57432\n",
      "  0.35111   0.39926   0.33184   1.2035   -0.21218   1.2316    0.58557\n",
      " -0.40531   0.37376   0.16584   0.56948  -0.13898  -0.29062   0.56082\n",
      " -0.94112 ]\n"
     ]
    }
   ],
   "source": [
    "print(word_to_vec_map['bottle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Â∞ÜÂè•Â≠êËΩ¨Âåñ‰∏∫ÂÖ∂ÂçïËØçÂØπÂ∫îÁöÑÁ¥¢Âºï\n",
    "def sentences_to_indices(X, word_to_index, maxLen):\n",
    "    \"\"\"\n",
    "    maxLen -- maxinum number of words in a sentence.\n",
    "    \"\"\"\n",
    "    m = X.shape[0] # number of training examples\n",
    "    X_indices = np.zeros((m, maxLen))\n",
    "    for i in range(m):\n",
    "        words = X[i].lower().split()\n",
    "        for j in range(len(words)):\n",
    "            w = words[j]\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "[[155345. 225122.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.]\n",
      " [220930. 286375. 151266.      0.      0.      0.      0.      0.      0.\n",
      "       0.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play football\"])\n",
    "print(X1.shape)\n",
    "X1_indices = sentences_to_indices(X1, word_to_index, 10)\n",
    "print(X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "print(word_to_vec_map['a'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building embedding layer in Keras\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    wants:\n",
    "    embedding layer has the weights' shape of (vocab_size + 1, word2vec_dim)\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map['a'].shape[0]\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # assign\n",
    "    for w, i in word_to_index.items():\n",
    "        emb_matrix[i, :] = word_to_vec_map[w]\n",
    "        \n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False.\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    \n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer.\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # set the weights of the embedding layer to the embedding matrix.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20635   0.2606   -0.094723 -0.73396   0.72598   0.5099   -0.39352\n",
      " -0.45703   0.49335   1.3791    0.10285   0.14997   0.41506  -0.19039\n",
      "  1.0527    0.16514  -0.16717   0.8092   -0.97394  -1.753     0.34632\n",
      " -0.053064  0.33046  -0.021036 -0.78655  -1.0088   -0.30341   1.6766\n",
      "  0.90808  -0.39309   1.2131    0.21588  -0.87778   1.3756    0.57432\n",
      "  0.35111   0.39926   0.33184   1.2035   -0.21218   1.2316    0.58557\n",
      " -0.40531   0.37376   0.16584   0.56948  -0.13898  -0.29062   0.56082\n",
      " -0.94112 ]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.get_weights()[0][81641])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "<img src=\"images/emojifier-v2.png\" style=\"width:700px;height:400px;\"> <br>\n",
    "<caption><center> **Figure 3**: Emojifier-v2. A 2-layer LSTM sequence classifier. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    # input tensor\n",
    "    sentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
    "    \n",
    "    # Create the embedding layer\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through the embedding_layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # Propagage the embeddings layer through an LSTM with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequence\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    \n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Propagate X through an LSTM layer with 128-dimensional hidden layer\n",
    "    # Be careful, the returned output should be single hidden state, not a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    \n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Propagate X through a Dense layer whith softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(5, activation='softmax')(X)\n",
    "    \n",
    "    # Add a softmax activation\n",
    "    # wonder if is necessary?\n",
    "#     X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(sentence_indices, X)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train&test data\n",
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/test.csv')\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 0s 614us/step - loss: 0.0339 - acc: 0.9924\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 689us/step - loss: 0.0190 - acc: 1.0000\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 682us/step - loss: 0.0161 - acc: 1.0000\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 758us/step - loss: 0.0148 - acc: 1.0000\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 742us/step - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 750us/step - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 750us/step - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 712us/step - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 780us/step - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 758us/step - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 879us/step - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 848us/step - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 841us/step - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 697us/step - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 758us/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 712us/step - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 727us/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 795us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 811us/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 871us/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 674us/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 636us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 682us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 667us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 682us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 735us/step - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 795us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 735us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 811us/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 682us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 727us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 652us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 758us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 788us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 735us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 758us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 735us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 803us/step - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 841us/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 727us/step - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 735us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 803us/step - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 758us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 758us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 780us/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 773us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 712us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 765us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 811us/step - loss: 0.0017 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2f022198>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_oh_train, epochs=50, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 411us/step\n",
      "\n",
      "Test accuracy =  0.9464285629136222\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç¥\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(sentences_to_indices(np.array(['i would like to have some noodle']), word_to_index, maxLen))\n",
    "print(label_to_emoji(np.argmax(prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
